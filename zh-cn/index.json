[{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\nAlphabet 字符串的排序跟传统排序一个重要的区别就是我们可以提前知道字符串的字符集。许多算法都是基于字符集的概念上来的。\n1interface Alphabet { 2 // Alphabet(String s); 3 char toChar(int index); 4 int toIndex(char c); 5 boolean contains(char c); 6 int R(); //字符集的字符数 7 int lgR(); //所需占用的空间(bits) 8 int[] toIndices(String s); 9 String toChars(int[] indices); 10} 例如，传统一个char我们需要用16个bit来表示，但是如果我么知道当前的字符是“0-9”，那么完全可以用4个bit来表达每个字符。 从这个意义上来说，Alphabet压缩了字符范围。 比较传统意义上的排序，我们只能基于compareTo来区分其大小。\nKey-indexed Counting 当Key的数量很小时，对Key索引后计数的方式排序是高效的，并且这种排序是其他排序的基础。 一个常用的场景就是老师对所有的学生按照班级排序。由于班级数是很小的，所以适用于Key-indexed counting. 算法步骤：\n对于每个key统计其出现的次数，例如统计每个班有多少个学生。 我们用数组int[] count来保存统计信息，注意：如果key的值是r,保存为count[r + 1].(也就是说我们将count[0]留空) 将计数转化为索引位置。按照顺序，第一个key的起始位置是0，第二个key的起始位置是count[第一个key], 第三个key的起始位置是 第二个key的起始位置 + count[第二个key]， 依次类推 由上面看出，我们将key保存时都在count中后移了一位(count[r + 1]), 所以实际上第i个key的起始位置为loc[i-1] + count[i]; 如果我们不使用额外的数组int[] loc来保存起始位置，而是直接在count上计算，那么这种转化就是 1for (int i = 0 ; i \u0026lt; R; i++) { 2 count[i + 1] += count[i]; 3} 分配数据。最后我们创建一个辅助的数组T[] aux. 对原始的待排序数组一个个的取出来，根据其key值找到对应的索引位置方式aux中，并将count[r]的值+1 1for (int i =0; i \u0026lt; N; i++) { 2 aux[count[a[i].key()]++] = a[i]; 3} 将结果从aux复制回a Key-indexed counting算法使用8N+3R+1的数组访问。并且该算法时稳定（Stable）的 注意：在sort算法中，到目前为止，只有Insert sort和merge sort是稳定的。 所谓稳定是指对于相同的key, 排序后是否保留的其原来的前后关系。\n从分析上来看，Key-indexed counting打破了排序算法的NlogN的下限。为什么可以做到，这就是因为传统算法中我们只能通过compareTo来比较两个元素来排序，而key-index counting则完全不需要进行比较，当 R是个常量时，我们就等于做到了线性时间排序。\n在字符串排序中，Alphabet的R至关重要\nLSD (Least-significant-digit) LSD是从右至左的顺序来检查字符串并排序。 这个方法针对的是需要排序的字符串长度相同。例如统计高速路上一段时间内有多少辆车，方法就是将这段时间内的车牌排序后去掉重复的。类似的还有电话号码，银行账号，ip地址等。 利用Key-indexed counting算法，假设字符串长度是w, 我们从最后一个字符开始每次执行key-indexed counting排序，执行w后即完成排序 LSD稳定的对固定长度的字符串排序。\nMSD (Most-significant-digit) MSD则是按照从左至右的顺序来检查。 MSD之所以吸引人是因为其可以不需要检查所以的字符就完成排序。这个算法与Quick Sort比较类似，因为他们都是将元素分段然后递归的完成排序。区别在与MSD使用第一个字符来完成分段，而qs则可能需要对所有的元素进行比较来完成分段。 MSD有两个方法， 一种是对每个字符进行逐步分段， 另一种则是对字符分成三段，小于，等于，大于基准字符\nMSD即是对LSD的扩充，使之不仅局限于固定长度的字符串。 MSD的核心就是从左至右，对第一个字符进行key-indexed counting排序，然后对每个key,分别做递归的对每个subarrays排序。 对于MSD我们要特别注意当到达字符串结束的时候。 假设对第d位进行排序，此时一个字符串已经结束，我们当然希望这些已经结束的字符串保留相对位置，并且在后续的sub排序中排除掉。\n在charAt(String s, int d)中如果d \u0026gt;= s.length()则代表该字符串已经中止，我们返回-1，否则我们返回s.charAt(d)。 改造count. 由于返回的索引是-1\u0026hellip;R-1, 我们对其+1后变成0\u0026hellip;R, 长度为R+1. 其中0代表当前已经中止的字符，1代表Alphabet中第一个字符。 由于我们还需要一个位置来表示最开头的位置为0，所以实际上count的长度为R+2. 每个字符对应的索引为charAt(d) + 2. count[0]固定为0，count[1]代表已经中止的字符的数量，count[2]代表第一个字符的数量。。。。count[R+1]代表第R个字符的数量 对count进行坐标转换，count[0]代表起始位置（固定为0），count[1]代表第一个字符的开始位置\u0026hellip;, count[R]代表第R个字符开始的位置，count[R+1]不使用 按照count对数组进行分散处理。此时索引是count[charAt(a[i], d) + 1] 由于我们在上面分发是都是对count进行自增处理，所以分发完成后相当于count[0] = 原来的count[1]. 也就是说count[0]代表第一个字符的开始位置。以此类推。所以接下来对sub进行递归时，我们直接从0开始遍历count,分别找到sub的上下标 1public class MSD { 2 private static int R = 256; 3 private static final int M = 15; 4 private static String[] aux; 5 private static int charAt(String s, int d) { 6 if (d \u0026lt; s.length()) { 7 return s.charAt(d); 8 } 9 return -1; 10 } 11 12 public static void sort(String[] a) { 13 sort(a, 0, a.length - 1, 0); 14 } 15 16 private static void sort(String[]a ,int lo, int hi, int d) { 17 //如果sub很小，选用插入排序YYDS 18 if (hi \u0026lt; lo + M) { 19 Insertion.sort(a, lo, hi, d); 20 return; 21 } 22 23 int[] count = new int[R + 2]; 24 //统计数量 25 for (int i = lo; i \u0026lt;= hi; i++) { 26 count[charAt(a[i], d) + 2]++; 27 } 28 //变换 29 for (int r = 0; r \u0026lt; R + 1; r++) { 30 count[r+1] += count[r]; 31 } 32 33 for (int i = lo; i \u0026lt; hi; i++) { 34 aux[count[charAt(a[i], d) + 1]] = a[i]; 35 count[charAt(a[i], d) + 1]++; 36 } 37 38 for (int i = lo; i \u0026lt; hi; i++) { 39 a[i] = aux[i - lo]; //aux是从0开始保存的 40 } 41 42 for (int r = 0; r \u0026lt; R; r ++) { 43 sort(a, lo + count[r], lo + count[r + 1] - 1, d+1); 44 } 45 46 } 47} 48 49//insertion sort 50public class Insertion { 51 public static void sort(String[]a, int lo, int hi, int d) { 52 for (int i = lo; i \u0026lt;=hi; i++) { 53 for (int j = i; j \u0026gt; lo \u0026amp;\u0026amp; less(a[j], a[j - 1], d); j--) { 54 exch(a, j, j -1); 55 } 56 } 57 } 58 private static boolean less(String v, String w, int d) { 59 return v.subString(d).compareTo(w.subString(d)) \u0026lt; 0; 60 } 61} Small Subarrays MSD之所以高效，在于对于大多数应用来说，我们只需要检查头几个字符就可以将其排序。另一方面，MSD快速将数组分割成多个sub来排序，但是这也是个双刃剑：我们会面临海量的sub. 因此sub的排序对于MSD是关键。 其他的递归排序，比如Quick Sort, Merge Sort也会有sub的问题，但是为什么MSD这个问题会更严峻呢？观察上面的算法，假设我们的R是256， 我们要对上百万的ASCII的字符串排序，同时这些字符串不会出现中间短一截的情况，那么最终我们会面临百万个长度为1的sub. 关键是，在MSD中我们是利用258长度的count来将其转化成索引的，而这个操作将为成为最耗时的部分。 所以对于MSD来说，\n如果sub的长度比较小（\u0026lt;15)我们必须用Insert sort来处理，从而避免这个问题。 谨慎选择R的范围，如果从R.ASCII切换到R.UNICODE，那处理的速度会同样指数上升 Equal Keys MSD的另一个缺陷就是对包含大量相同key的sub处理比较慢。如果这些字符串有大量的相同的字符，那么意味着我们在处理这些相同字符时无法对字符串进行再分割。同时key-indexed counting对于处理key相同也很慢，因为我们需要每次都初始化count，统计其个数然后转化为索引后才分配和拷贝回来，结果这些等于做了个寂寞。同样的，不仅如果大量字符串相同效率慢，大量前缀相同的字符串排序也会有这个问题。\nExtra space MSD需要一个aux长度为N, 不过这个不是特别关键， 重要的是需要一个count而这个count必须在每次递归中创建。\nThree-way quick sort 思路是将MSD和Quick Sort相结合。假设对i\u0026rsquo;th字符进行比较，将字符串分成小于，等于，大于的三组。只对等于的字符进行下一个字符的递归比较，而另外两组则重复当前位置i的处理。相较于MSD,MSD每次都会分割成大量的sub, 3-way qs只会分割成三个sub,所以其对于有大量相同前缀的字符串效率更高。\n1public class Quick3string { 2 private static int charAt(String s, int d) { 3 if (d \u0026lt; s.length()) { 4 return s.charAt(d); 5 } 6 return -1; 7 } 8 9 public static void sort(String[] a) { 10 sort(a, 0, a.length - 1, 0); 11 } 12 13 private static void sort(String[] a, int lo, int hi, int d) { 14 if (hi \u0026lt;= lo) return; 15 int lt = lo, gt = hi; 16 int v = charAt(a[lo], d); 17 int i = lo + 1; 18 while (i \u0026lt;= gt) { 19 int t = charAt(a[i], d); 20 if (t \u0026lt; v) exch(a, lt++, i++); 21 else if (t \u0026gt; v) exch(a, gt--, i); 22 else i++; 23 } 24 25 sort(a, lo, lt - 1, d); 26 if (v \u0026gt;= 0) sort(a, lt, gt, d+1); 27 sort(a, gt + 1, hi, d); 28 } 29} ","date":"2022-08-11","img":"","permalink":"/zh-cn/guide/algo-string-string-sorts/","series":[],"tags":["字符串排序"],"title":"Algo String String Sorts"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition Priority Queue之所以重要是因为有时候我们需要在获取部分数据时就进行排序后找到最大/最小的item。 此时我们不必要对这些元素完全排序，因为还可能继续的有元素添加进来。 在Prioirty Queue的基础上，又有了Heap sort。\n基本实现。 通过array我们可以实现最简单的PQ\nOrdered： 当我们插入一个元素时，我们在array按顺序插入，当delMin（）时将第一个取出即可 Unordered: 依次插入元素，当要取出时，遍历数组找到最小的元素取出 Heap Binary Heap能够有效的支持PQ操作。在Binary Heap中，key被存储在一个数组中，并且保证每个Key都大于或等于两个制定位置的KEY. 理解上，可以把Binary Heap想像成Binary Tree： 每个点的key都大于或等于其子节点的key. 由此，在BT上，向上走Key值总是更大的，向下走Key值总是更小的。 在Heap ordered binary tree中最大值总在root上。\nBinary Heap定义 Binary Heap就是一组Key按照完全Heap Ordered Binary Tree的顺序来安排，按照层级在数组中表示（注意：数组的第一位不使用） 由此，很重要的点就是对于每个节点其位置为k, 则其两个子节点的位置为2K, 2K+1; 其父节点的位置为 k/2\n一个完全的二分树的高度为lgN\n重要的操作 Swim 如果一个节点的值比父节点的值大，违反了我们对Binary Head的要求，因此我们要将该节点上移，如同Swim. swim的方式很简单，不断检查该节点(k)与其父节点(k /2 )，如果比父节点大则交换两个节点，直到根节点为止。\nSink 相反，如果一个节点比子节点小，我们就要将该节点下沉。Sink的方式：\n找到其两个子节点2k, 2k+1,并确定两个子节点中较大的那个节点的位置 将该节点与大的子节点交换。 循环直到比子节点大或者没有其他子节点。 1private void swim(int k) { 2 while (k \u0026gt; 1 \u0026amp;\u0026amp; less(k, k / 2)) { 3 exch(k/2, k); 4 k = k / 2; 5 } 6} 7 8private void sink(int k) { 9 while(2*k \u0026lt;= N) { 10 int j = 2 * k; 11 if (j \u0026lt; N \u0026amp;\u0026amp; less(j, j+1)) { 12 j++; 13 } 14 exch(k, j); 15 k = j; 16 } 17} Heap Sort Heap Sort的思路就是利用上面的PQ, 如果我们构造好PQ，每次将root取出来放入到队列末尾即可。 更准确的说，如果我们已经构造好了PQ, 每次我们将root根该PQ的最后一个节点交换，并将PQ的长度缩小一位。此时我们对放到root的节点重新做sink操作，如此循环。\n问题的关键变为如何第一步构造这个PQ。 最简单的办法是按照顺序从左到右一个个的进行swim操作。当遍历到最后一个元素时PQ构造完成。 但是还有更好的办法，是从右向左一个一个进行sink操作。 假设有N个元素，那么N/2 ~N区间的元素是Leaf，所以他们不需要进行sink, 我们可以从N/2开始向1遍历。\n","date":"2022-08-09","img":"","permalink":"/zh-cn/guide/algo-sort-priority-queue/","series":[],"tags":["排序"],"title":"优先队列"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n","date":"2022-08-02","img":"","permalink":"/zh-cn/guide/algo-graph-shortest-paths/","series":[],"tags":["graph"],"title":"最短路径"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n中心思想 最小生成树的围绕Cut property这个概念展开。\n所谓Cut property， 就是将一个图的顶点分成两部分，一条边如果连接的是不同部分的顶点，则这条边就叫cross edge 如果此时连接两部分的所有cross edge, 其中最短的这条就在mst上。\nPrim prim算法就是根据上面的定义来实现的。 如果我们把一个图的顶点想象成white , gray两色的点。初始时图中都是白点。\n当我们将一个白点染成灰色，此时可以找到白-灰间的所有cross edge, 最短的那条属于mst. 我们把这条边的另一头涂灰，同时将失效的cross edge去掉（方法就是在剩下的cross edge中去掉一头属于这个新添加的灰点的edge， 因为此时这些边的两头都是灰色，不再属于cross edge） 添加新的cross edge(就是查看这个新点的所有边，那些另一头仍然是白色的边)。 继续在这些边中找到最最短的边加入mst. 以此类推，知道所有的点都变灰 要实现上面点算法有几个要素。\n当新增一个点到mst时，能够找到所有新增的cross edge 能够去掉ineligible cross edge 在所有的cross edge中找到最短的那条 Lazy Prim boolean[] marked 来表示一个点是否已经被添加到mst(是否是灰色) Queue mst 代表mst的所有edge MinPQ pg 放置所有的cross edge 算法描述 在marked中将第一个点标记为true, 同时将该点的所有edge添加入pg pg中取出min这条，将edge的另一头在marked中标记为true, 同时将这条edge加入mst 遍历新点的所有edge，如果edge的另一头为unmarked的，则加入pg pg中取出min, 如果这条edge的另一头已经marked ,说明这条edge失效了，我们再取一条 循环直到所有点都处理完 算法缺点 缺陷就是在pq保存了cross edge以及大量失效的edge\nPrim改良 改良的核心就是在pq中去掉失效的边 另一方面，如果点v此时是非mst点，但是其通过cross edge连接到了mst, 无论此时有多少条，我们最终需要的都是最短的那条。 因此每当我们将要连通一个新点时，只需要将连到该点的最短的edge添加到pq。 如果有更新，比如原先只有两条cross edge连到这个点，现在由于增加了新点，多了几条边到这个点，我们也只需要保留最短的一条即可。\n由此数据结构调整\nedgeTo[v] 代表连接到v的最短的cross edge distTo[v] 代表当前mst到v的最短距离，即edgeTo[v]的weight, 初始为Double.POSITIVE_INFINITY marked[v] IndexMinPQ pq 索引即顶点的编号，val即cross edge \u0026rsquo;s weight, 最小的即是下条添加到mst的点 比较改良前，仍然用marked代代表已经添加的点，distTo[v]代表该点的最小weight，也就是edgeTo[v]中的weight edgeTo保存的就是最终的mst(不包括edgeTo[0]) 当新增一个cross edge, 如果该edge的权重小于distTo[v], 则在pq中更新\nKruskal算法 Kruskal的思路略有不同，但本质是一致的。将所有的边放到MinPQ里面，每次取出最小的一条，如果这条边与现有的mst的边不构成一个环，那么这条边就是mst的一条边，如此循环直到所有边添加完成。\n证明的话也很直观，如果一条边不与现有的mst的边构成环，那么说明其两端不会都是属于同一个cut,意即此边是cross edge; 又由于其是现有的最小的边，那么其属于mst\n算法思路 将所有边放入pq, 每次取出最小的一条 如果这条边的两端是否已经连通（UF）: 如果连通则取下一条 否则将这天边放入mst， 同时将两点union 与prim比较 两者都是贪婪算法的体现，prim从一个点开始，每次找出minimum cross edge, 新增一个点，然后继续添加 kruskal则从最短的edge开始一条条添加，只要不构成环。\n从效率上来说prim会更搞笑一点，因为kruskal需要使用UF\n","date":"2022-08-02","img":"","permalink":"/zh-cn/guide/algo-graph-minimum-spanning-tree/","series":[],"tags":["graph"],"title":"最小生成树"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\nGlossary Directed Graph / Digraph directed path directed cycle / simple cycle 数据结构 Digraph的结构与Graph类似，除了Adjacency List指保留 w-\u0026gt;v的连接， 而graph则同时保存w-\u0026gt;v和v-\u0026gt;w 另外Digraph增加了reverse()方法，其返回这个图的拷贝，只是将所有的w -\u0026gt;v 变成的v -\u0026gt; w. 这么做是因为有时候在处理中需要找到所有到v的边\nReachability 同样适用DFS来完成。 应用： 一个应用场景就是内存回收，通过定期扫描现有的引用，通过DFS将所有连接的引用标记出来， 剩下的未标记的引用则可以被回收\n寻找路径 跟Graph相同，Digraph使用DFS查找路径， BFS查找最短路径\nCycle \u0026amp;\u0026amp; DAGs (Directed Acyclic Graphs) 在Digraph中要判断是否有环，如果没有计算机的帮助是很困难的。一个Digraph中可能有大量的环，我们主要的关注点在于部分的环，或者有时只关心到底这个图有没有环（DAG） 一个应用场景就是作业调度，其包含各种限制，其中一个重要的限制就是每个job的先行条件(precedence constraints) 。 以学生选课为例，有些课必须要已经选修了特定的课程才能选择，如果进一步限制一个学生同时只能上一门课，那么这个问题就演变为Precedence-constrained Schedule(优先约束的调度) -\u0026gt; 也就是建立于Topological Sort(拓扑排序)之上\nTopological sort 在Digraph中，将所有的点按照边的指向从先到后排列。 典型的需要Topological sort的应用有： 作业调度， 课程调度，继承（Java class）, SpreadSheet, 符号连接\n如果图中有环，例如 a,b,c，那么我们无法按照topological sort的方式排列， 由此推导出很重要的一点就是对digraph中cycle的判断。 Digraph的环判断思路： 当我们用DFS探索的时候，将当前路径的点保存在数组中， 如果我们探索一个新的点，而这个点已经保存了，则发现了环。当一条路走完，我们往回退的过程中则把这条路上的点从保存的数组中抹去，然后在新的方向继续重复操作。 代码层面\n1boolean marked[]; 2int[] edgeTo; 3boolean[] onStack 4Stack\u0026lt;Integer\u0026gt; cycle; 5void dfs(Digraph g, int v) { 6 onStack[v] = true //每次调用dfs，相当于我们走进了一条新路， 7 marked[v] = true; 8 for (int w : g.adj(v)) { 9 if (hasCycle()) return; 10 if (!marked[w]) { 11 edgeTo[w] = v; 12 dfs(g, w); 13 } 14 else if (onStack[w]) { //当发现点保存过，则发现了环 15 cycle = new Stack\u0026lt;\u0026gt;(); 16 for (int x = v; x != w; x = edgeTo[w]) { 17 cycle.push(x); 18 } 19 cycle.push(w); 20 cycle.push(v); 21 } 22 } 23 onStack[v] = false; //精髓，当退出dfs时，相当于一条路探索完了，我们把这条路上的点抹去 24} 一个有向图可以拓扑排序当且仅当这个图时无环图（DAG） Preorder / PostOrder/ Revese PostOrder Preorder: 在调用dfs前将点放入队列 Postorder: 在调用dfs后将点放入队列 Reverse Postorder: 在调用dfs后将点放入栈 而topological sort就是调用reverse postorder得到的序列\njob-scheduling 的处理流程 明确task和所有的前置条件 通过判定是否有环来明确是否存在解决方案。 如果有环，可以考虑如何消除环 用Topological sort来生成调度计划 Strong Connectivity 在有向图中，两个点是强连接指两个点相互连通。由此可以看出，两个点是否强连接就是看这两点是不是在一个环里。 强连接的平等特性促使其有以下特征\n自反性： 每个点对于其自身都是强连接 对称性： 如果w强连接到v, 则v也相连接到w 传递性 Strongly connected components 一个图中，每个包含最大相连接的点组成的字图就叫Strongly component. 图中的每个点都只属于一个Strongly Component. 一个DAG有V个Strongly Component. 注意：Strongly component 关注的是顶点而不是边。因此每个点只能属于一个Strongly Component中，而一个边可能连接一个，也可能跨区域连接 对于无向图，我们用CC来研究，对于有向图，我们用Strongly Component来研究。 Strongly components对于研究有向图的构成很有用，每个component中的点都是强关联的。例如对于教材，作者根据topics来分组。 对于软件开发，可以用来划分模块。更好的例子是食物链，通过捕食者与被捕食者的连接我们可以划分出不同的生态圈。\nKosaraju\u0026rsquo;s 算法 通过Kosaraju算法来得到Strongly Components,步骤如下\n通过DepthFirstOrder对一个Digraph的reverse得到其reverse post order(GR) 在Digraph上运行dfs, 但是顺序是按照上面得到的GR来 由于是对每个顶点一个个分别执行dfs, 如果在一个顶点中执行dfs递归碰到的所有点就是该点的Strongly Connectity. 简单理一下为什么可以得到： 在探索的时候，对于一个环，可能会有一个或多个点点进入这个环，然后这个环从一个或多个点走出去。假设这些进入的点和出去点都属于其他的Strongly Component，那么我们要在一次dfs中只访问这个环中的内容，唯一的办法在这个dfs之前已经探索过所有的出口的点，执行dfs探索完这个环，最后探索所有入口的点。如果a,b代表入口点，C代表环，d,e代表出口点，图a/b-\u0026gt;C-\u0026gt;d/e的探索顺序必须是d/e-\u0026gt;C-\u0026gt;a/b, 其他顺序都会导致错误，例如我们探索完d/e后探索a,dfs会将aC一起探索了，从而得到aC是Strongly Component的错误结论。 进一步来说，如果每个Strong Component都看作环，只要确保所有的环都是先探索出口即可满足条件 关键就是探索的顺序必须是出口的点-\u0026gt;环本身- post order的特征是远的点在前，用这个顺序可不可以呢。答案是不行。\n在有环的时候，如上面a/b -\u0026gt;C -\u0026gt;d/e的图中，如果此时在C里面进行dfs探索，无法保证下一步是先探索d/e还是继续在C中走，从而出现可能先把C的点都探索完了，然后再探索d/e的情况。（可以想像成环在起点断开了，此时环的剩余点也构成了一道分支，到底dfs走哪一路就跟图中点的位置相关了）。\n对于有环的有向图，post order的顺序是不确定的，会出现不相关的两个环，其中一个点在前，然后不相关的环的点在后的情况。 但是post order可以保证两点\n如果一个点在环中，从这个点有路径走出，那么无论安什么方向遍历，这个点一定排在走出的点后面。 如果一个点在环中，从这个点有路径走入，那么无论安什么方向遍历，这个点和点所在的环一定排在走入的点前面。 （假设从环的外面开始遍历，走到入口的点，那么必然会继续走进环，自然就会走完环然后逐步退出， 此时环点在前，入口的点在后；如果是环开始遍历，由于入口点没法走到，所以也是环点在前，入口点在后） 由第一条可以看出，如果一个环有多个出口，我们用post order无法保证所有的出口的点都在环的点前面。 但是如果先把图反转，对于环来说还是环，但是对于所有的入口就都变成了出口，而出口就变成了入口。那么此时post order的第二条规律可以保证环的点在所有入口点的前面，反之则变成出口的点在环的点前面，达成目标。\n对于order, 一定参看这篇文章 官方证明 首先证明在一组递归dfs中碰到的点都是在一个Strongly Component中。 这里使用反证法。假设点v是与dfs(G, s)强连接但是又不能被dfs(G, s)访问到的，那么必然v在之前的组的dfs遍历中被标记了。由于s,v是强连接，那么之前组的dfs访问到v的时候必然会访问到s, 因为那时s也没被标记。如果那样，s点开始dfs组会被跳过。由此产生矛盾。 接着证明如果dfs(G, s)访问到点v, 那么点v与点s是强连接的。由于dfs(G,s)能够访问到v,说明有路径s-\u0026gt;v， 那么只需要证明存在v-\u0026gt;s的路径即可。换句话说就是在GR(G的reverse)中存在s-\u0026gt;v。 由于我们是按照GR的RPO顺序遍历的，s在v的前面，那么在GR中dfs(G, v)先于dfs(G, s)结束，于是可分为那种可能 dfs(G,v)先于dfs(G,s)开始，并且在dfs(G, s)开始调用前结束。 dfs(G,v)后于dfs(G,s)开始，并且在dfs(G, s)结束前结束。 由于G中明确了又s-\u0026gt;v, 那么GR中必然有v-\u0026gt;s, 所以1不可能，由2可以推测出在GR里面有s-\u0026gt;v。 重新回顾Reachability 对于无向图，通过CC我们判断出有多少个Components,而Component内是相连的，不同的Component是不相通的，从而可以判断出对于任何任何点v,w是否相连 对于有向图，通过KosarajuCC我们可以得到Strongly Components, 从而判断v,w是否强连接，即v-\u0026gt;w \u0026amp;\u0026amp; w-\u0026gt;v。但是对于不是强连接的点，我们无法判断其是否想通 由此引出了一个判断任意点是否相连的问题 无向图通过CC, 通过linear time process, 可以在constant time得到查询结果 有向图却不能通过KosarajuCC来同样做到 传递闭包 Transitive Clousure 有向图G的传递闭包指包含所有G的顶点，如果从G有v-\u0026gt;w， 那么传递闭包中则存在v-\u0026gt;w的边。 一个有向图可能是稀疏的，但是其传递闭包可能是紧凑的。 构造出传递闭包，其实就实现了有向图中两点连通的判断问题。\nTransitiveClousure 算法很简单，就是对每个点分别执行DirectedDFS来预处理，判断时直接检查对应的值即可。 这个算法问题在于需要大量的空间，对V个顶点的图需要V*V的空间 因此对于巨型图，这个算法是不实际的。 需要指出，现在还没找到可以快速解决的这个问题的算法。\n","date":"2022-07-29","img":"","permalink":"/zh-cn/guide/algo-graph-directed-graph/","series":[],"tags":["graph"],"title":"有向图"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n所谓图，就是有一些顶点和边组成。\n基本术语 path degree subgraph simple path cycle / simple cycle connected acyclic tree: an acyclic connected graph forest spanning tree (of a connected graph) 数据结构 在考虑图的数据结构时，需要满足两个方面\n空间上能够存储 图的方法（此处主要指adj()）能够在有限的时间完成 由此，对于以下方式\n二维数组，g[i][j] = true代表i -\u0026gt;j ： 太耗费空间；无法提供paralle edge的表达 数组Edge[] : adj()需要遍历整个图才能得到 Array of Adjacency lists : Bag[], 每个元素保存从该点出发的边 dfs 有了图的基本数据结构， 第一个重要的算法就是判断两个点是否相连。 实现这个目的，我们可以用Union Find算法， 其思路就是每个点通过edge找到与其相连的最小的点。 判断两个点是否相连就是看其最小点是否相同。 更好的方式使用Depth-first search\n迷宫探索 如果以迷宫探索来类比，\n从入口开始，牵一根线 走过的路径和交叉点都标记出来。 当碰到标记过的交叉口，就顺着线退回来 如果在一个交叉点没有位标记过的路径可以探索，就继续回退 Union Find可以确定两点是否连接，但是无法找到连接的路径。\n查找路径 DepthFirstPath 核心就是保存了每一次dfs中联通的edge的记录。\n1void dfs(Graph g, int v) { 2 marked[v] = true; 3 for (int w: g.adj(v)) { 4 if (!marked[w]) { 5 edgeTo[w] = v; //由于是无向图，因此edgeTo[w] = v可以代表为从v-\u0026gt;w 6 dfs(g, w); 7 } 8 } 9} 要查找路径，就是从终点开始反推，直到起点\n1Iterable\u0026lt;Integer\u0026gt; pathTo(int v) { 2 if (!hasPathTo(v)) return null; 3 Stack\u0026lt;Integer\u0026gt; path = new Stack\u0026lt;\u0026gt;(); 4 for (int x = v; x != s; x = edgeTo[x]) { 5 path.push(x); 6 } 7 path.push(s); 8 return path; 9} bfs Breadth-first search 处理单点的最短路径 dfs类比一个人探索迷宫，而bfs则好比一群人探索。当碰到分支，则分出部分人去探索，当两个分支合并，则这些人也随之合并。 dfs的本质是使用stack（系统提供了递归）来保存探索的路径，而bfs则使用fifo(queue)\n1void bfs(Graph g, int s) { 2 Queue\u0026lt;Integer\u0026gt; queue = new Queue\u0026lt;\u0026gt;(); 3 marks[s] = true; 4 queue.enqueue(s); 5 while(!queue.isEmpty()) { 6 int v = queue.dequeue(); 7 for (int w : g.adj(v)) { 8 if (!marked[w]) { 9 queue.enqueue(w); 10 edgeTo[w] = v; 11 marked[w] = true; 12 } 13 } 14 } 15} Component Connectivity 探索了图中的点的联通和路径，接下来就需要探索一个图中有几个component. CC的实现很巧妙，当用DFS探索时，如果一个图是完全联通的，那么我们在不断的递归过程中一定能够探索完所有的点。 反之，如果递归调用一次dfs后，我们发现graph中仍然有点点状态是unmarked， 那说明这个点属于另一个component 我们用id()来代表不同的component\n1class CC { 2 boolean[] marked; 3 int[] id; 4 int count; 5 6 CC(Graph g) { 7 marked = new boolean[g.v()]; 8 id = new int[g.v()]; 9 for (int s = 0; s \u0026lt; g.v(); s++) { 10 if (!marked[s]) { 11 dfs(g, s); 12 count++; 13 } 14 } 15 } 16 void dfs(Graph g, int v) { 17 marked[v] = true; 18 id[v] = count; 19 for (int w : g.adj(v)) { 20 if (!marked[w]) { 21 dfs(g, w); 22 } 23 } 24 } 25} Cycle Detection \u0026amp;\u0026amp; Two Color Cycle DFS是探索图的一个核心。有了dfs, 要探索图中是否有环，思路就是在dfs递归过程中，如果发现某个点已经标记过，而这点又是从连接的另一头又不是当前探索的点， 那么就说明有了环。 这里涉及到了三个点： 探索的点v, 走到这个点的出发点u, 以及可以从v出发去探索的点 w. 如果此时发现w已经被标记过，而且u != w, 那么就可以明确，绕回来了。\nTwo Color two color解决的是bipartite graph(二分图)的问题，说白了就是图中的任何一个点，如果是黑色，那么与其相邻的点都是红色。 作用：\n二分圖廣泛應用於編碼理論中，尤其常應用在收到從通道傳來的訊息之後解碼過程中。常見的例子有坦納圖和因子圖。坦納圖是一個二分圖，其中一個獨立集蒐集所有\u0026gt; 的一個碼字裡可以放數碼的位置，另一個獨立集則包含一些可以放數碼的位置的組合，其中每個組合代表一個碼字所該符合的限制──那些位置的數碼加起來總和是 0[34]，而連邊就代表了屬於。因子圖則與低密度奇偶檢查碼及渦輪碼的機率解碼中所用到的貝氏網路密切相關[35]。\n在電腦科學中，佩特里網是一個數學工具用來分析及模擬並行計算，它將系統模擬成一個有向二分圖，其中一部分的頂點被稱為「位置」節點，包含一些資源，另一部分的頂點被稱為「事件」節點，消耗或生產資源，節點和邊之間的關係還有一些限制，這些限制來自系統本身的限制。佩特里網藉由有向二分圖的性質讓系統的行為可以用數學來證明，並且讓系統的模擬容易被執行[36]。\n在射影幾何中，列維圖是一個二分圖，描述幾何構形中點跟邊的關係。頂點的兩部分分別對應到構形的點跟邊，圖中兩頂點之間連邊若且唯若構形中的邊通過點，因為兩條邊頂多交於一點，或者說，兩點決定唯一一條邊，所以列維圖中不存在長度為 4 的環作為子圖，換言之，列維圖的圍長大於等於 6[37]。\nSymbol Graphs 已电影目录为例子，包括电影名称和演员，那么这些电影名称和演员就可以组成一个Symbol Graph, 注意这个graph是二分图，因为不会有两个电影或两个演员相连。 Symbol graph其实就是一个普通的graph + 数组。其中数组保存了symbol（比如电影名和演员名），数组的索引就是graph中的点。\n应用 DegreesOfSeparation Graph的一个重要应用就是研究两点之间的分离度，例如分析两个人之间通过多少人才联系起来的从而得到这两个人的亲密度。 这个问题其实可以归结为寻找两点之间的最短路径，因此Symbol Grapsh配和BFS即可实现。\n总结 在Graph的处理中，我们通过构造函数讲图进行预处理，创建出可以快速得到结果的数据结构，这种思路只得学习。 图的处理的关键是DFS和BFS,在两个搜索方式的基础上，可以实现DepthFirstPath/BreadthFirstPath， 来过去两点间连接路径。 CC则是在DFS上明确Graph中的Component Cycle是在DFS上判断是否图中有环 TwoColor是在DFS上判断图是否为Bipartite Graph 由Graph + array 则可包装出Symbol Graph Symbol Graph + BreadthFirstPath可以研究两点间的分离度 DegreesOfSeparation ","date":"2022-07-29","img":"","permalink":"/zh-cn/guide/algo-graph-undirected-graph/","series":[],"tags":["graph"],"title":"无向图"},{"categories":["生活琐碎"],"content":"农历年过完了，算是开始了新的一年。\n新年快乐！\n可惜，这个年过的有点心力交瘁，所以记录下来，作为将来的回忆。\n整个年的波折是从大年三十开始的。回家里团年，一大家人吃吃喝喝，本来事件很开心的事情。可惜安安从下午开始精神不好，到了晚上发现有点烫，于是立马回自己家。到了家用体温计一测，39度，有点坐不住了，想了想，跟老婆商量了一下，还是带他去医院稳妥点。 晚上10点左右赶到华西，医生安排查血，做核酸检查。 大概半个小时后查血结果出来了，有感染，开了头孢，还有止咳退烧药。 继续在医院发热门诊的帐篷里继续来回逛——还得等核酸的阴性报告，不然不放人。12点终于有结果了，找医生盖章走人。\n人生第一次大年在医院度过了。我以为这算是很惨了，没想到其实这才是开始。\n回到家大约1点，想到安安肯定不舒服，晚上指不定如何闹腾，干脆让安安睡在旁边。\n大年初一，一夜辗转，第二天醒来，发现嗓子剧痛无比，我感觉自己中招了，加上老婆前两天开始就有点嗓子不舒服，算是四个人倒下了三个。想到今天大年初一，去医院只能去急诊，不知道花多少时间，家里只有老婆去搞定两个宝宝，其中一个还感冒，有点吃力，于是赶忙打电话给老江，咨询了可以吃啥药。万幸，楼下药房还正常营业，连忙买好阿莫西林。到了中午，身体开始酸软无力，一测体温有点高。 想了想还是在家里稳一稳吧，没那条件去医院。\n扛吧，下午开起暖炉，抱着安安在沙发上休息了几个小时。安安算是彻底趴了，平时一刻都不停的，现在只能躺在我大腿上偶尔哼哼两声。 其实我现在对安安和自己到不是特别的担心，因为根据以往的经验，这种细菌的感染我自己差不多第二天就能恢复，安安今天体温已经还是下降了，第三天应该就能不烧了。\n下午丈母娘来了一趟，帮着打扫了一下，给小孩弄了晚饭。\n晚上决定把安安丢回小床睡，也上我们两口子能稍微好好睡觉。\n大年初二，自己感觉人精神了很多，昨晚出了些汗，体温也正常了，算是松了一口气，就是嗓子继续疼，吞口水都疼的那种。安安也在逐渐恢复，还是愿意吃东西。我和老婆现在开始担心平平，不知道平平能不能躲过一劫。下午情况继续向好，老婆甚至来了兴致在盒马上点了车厘子还有一些馒头豆沙包之类。 晚饭一家人继续喝粥，由于嗓子疼，对我们来说倒是挺不错了。\n下午开始，平平有点反常，特别爱哭，而且特别黏着我，甚至到只要我抱的地步。我其实不是很想抱他，毕竟我也算传染源了。到了晚上睡觉发现不对劲了，平平睡不熟，大概过一个小时左右就会大哭大闹，而且表现的很矛盾，既要我抱，抱住他他又老是想推开我。折腾到凌晨三点，想了想，还是送医院吧。潜意识里，因为平平上个月做了动脉导管闭合的手术，我怕封堵器会不会跑了（虽然觉得可能性太低太低）。\n凌晨四点来到华西，我本来想这次平平没发烧，应该要不了多长时间，结果医生说听着肺不是很好，去拍个片吧。跑去放射科，叫醒医生。当时在放射科等医生来的时候，那个嗓子的疼啊，还有点咳嗽，顿觉无比难受。还好假期人不多，报告不到一个小时拿到了，再拿去给医生看，医生说有点炎症，但是还好，给开了药。嘱咐我注意观察他的体温，特别是发烧之类。\n回到家早上7点半，快速给平平吃了药，上床睡觉。\n大年初三，早上两个小孩大约十点就醒了。可我们两个大人实在有点起不来，于是给他们喝了奶放到客厅去玩。我们接着眯到了11点。觉得自己精神还不错，除了还是嗓子疼。 平平也安静了许多，虽然我还是很担心，感觉他这两天不怎么睡觉肯定是不舒服。不过从白天观察来看也还好。我差不多隔两三个钟头给他们测一次体温，都正常了。丈母娘中午又过来了一趟，帮着收拾了一下。\n晚上，我们决定不在家待着，去环球中心吃点，换个心情。找了家港式餐厅，两小只虽然胃口差了点，但好歹也吃了一些。回到家大约十点过，结果我给平平换尿不湿时发现他有点发热，一测40度。哎，可怜的平平，你最终也没躲过。把安安之前开的抗生素匀了一包给平平吃了。想着再怎么今晚观察一下，不行明天送医院，毕竟发热去一次就得做核酸被关几小时，伤不起啊。\n平平继续焦躁不睡觉，算了，我再次稳不住。看了看钟，凌晨三点半，要不还是去医院吧。又赶忙和老婆收拾东西，到华西快5点了。结果一到发热门诊，门口的护士给弄傻眼了，告知我，我家前两天已经有弟弟发热了，今天哥哥来就算是群体发热，医生会根据结果决定是否要求我们在医院呆24小时做二次核酸才放人，而且这个留观我们不能拒绝，会收几百块的留观费，当然该有的治疗可以正常进行。我说我们什么都没带，吃饭也不好解决。护士说可以让家人送，至于其他的就是我们自己的问题了。当然，最后人家还是给了个选择，由于现在还没登记，我们可以选择不在他们这看病，现在走还来得及。 我问她这个是不是全市统一的规定，告知我是医院根据指导意见定的。我想了想，还是留着吧，被关24小时好歹还能治疗。最多让家人送吧。\n又是查血，核酸，这次还开了个肺炎抗体的检查。7点过拿着查血和核酸结果找医生，医生给开了抗生素。肺炎抗体结果不知道什么时候出，医生说如果是阳性还得来复诊。 盖章走人，医生没提留观24小时的事，万幸万幸。\n回家8点过，立马喂药睡觉。\n大年初四，早上十点过，我妈敲门，救兵到了。小孩交给他们，我立马继续睡。可惜嗓子太恼火了，睡的不是很好。基本上睡不着。中午吃点东西，下午继续休息。 晚上家里有个饭局，我本来不想去，他们说都订好了，我想要不然也带老婆换换心情。菜肴很丰富，可惜我没胃口，嗓子很不舒服。 吃饭途中看到平平的报告出来了，果然阳性，于是和老婆草草了事，回家收拾东西，准备再带平平去一次医院。\n这次家里有人，于是终于和老婆两人一起带平平过来了。虽然还有点发烧，但是昨天已经做了核酸了，就不用那么折腾，直接去看医生。医生开了阿奇霉素，让吃三轮。搞完差不多11点。开车回家。晚上继续入睡困难，感觉喉咙老被刺激。半夜起来洗了次鼻子，稍微好点，终于迷迷糊糊入睡。\n大年初五，两小只算是告一段落，安安已经恢复的七七八八， 平平也还可以，还是有点胃口。今天准备修修大人。和老婆挂了耳鼻喉科的号，同一个医生。老婆开了一大堆药，到我这就让我查了个血。结果拿到，医生说感染有点严重，得输液。输液得有48小时内的核酸报告。 得得得，万事皆核酸。 立马去排队做。 到了晚上报告还没出来，只能用备选方案，跑了几家药房终于买到了莫西沙星先顶一晚。\n大年初六，核酸报告终于拿到了，我妈上午回去了。丈母娘他们过来顶下午。我下午去医院输液，晚上9点半才回家。\n大年初七，我开始上班，老婆带平平一医院复查之前做的动脉导管未闭手术的情况，保姆也终于返工了。\n总结一下：\n这个春节确实大大超出了我的想象，曾想着晚上看看春晚，逛逛锦里之类，结果一件都没实现。但是另一方面，这个春节也确实是测试我们在没有外援的情况下极限在哪里。所以从这个角度还来说还是不错，本身带两个宝宝就是一件很累人的事，再加上两个大人都生病，情况就更严峻了，不过两口子互相帮助，也算是跌跌撞撞走了很远。\n给自己的经验：\n下次碰到大人和小孩都生病的情况还是得提早找援兵。 后面换房子必须考虑到足够的空间，特别是如果有小孩生病了，不说避免相互传染（我觉得避免不了），但是要尽力做到一个大人晚上照顾孩子而不影响另一个大人的休息。 小孩生病后特别希望跟大人一起睡，所以下次搬家要将宝宝的房间布置成大人也能临时睡几天。 还是得加强身体素质啊，归根结底爸爸妈妈必须要多保重自己才能照顾好两小只。 ","date":"2022-02-07","img":"","permalink":"/zh-cn/posts/spring-festival-2022/","series":[],"tags":["life","kids"],"title":"Spring Festival 2022"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n由于二叉树严重依赖输入的随机性，因此我们需要平衡二叉树（红黑二叉树）来实现能够自动调整树的结构以达到平衡。\n2-3查找树 在BST中，每个节点都是2-nodes, 也就是说节点只有一个值，其可以有2个字几点，因此在这里引入了3-nodes的概念，即一个节点有两个值，可以有三个子节点。可以想象成3-nodes就是两个2-nodes节点拼在一起。\n定义： 一个2-3 查找树中每个节点要不然空，要不然\n是一个2-nodes: 该节点有一个值和左右两个子节点，左子树也是一个2-3查找树，并且其值都比该节点小，右子树也是一个2-3查找树，并且其值都比该节点大 或者是一个3-nodes: 该节点有两个值和三个子节点指针。左右子树根2-nodes一样，中间的子树的值介于该节点两个值之间 一个完全平衡的2-3 查找树就是指其所有的空节点到根的距离都相同。\n查找 2-3查找树中查找一个值根BST类似，都是递归的方式在对应的子树中去寻找，直至找到（命中）或者到达空节点（未命中）\n2-3查找树之所以能自动调整平衡，关键在于当往3-nodes中在加入一个值后其变成4-nodes, 而一个4-nodes可以拆成一个平衡的ts， 即中间的节点作为parent, 两边的节点作为左右自己点。因此对于2-3查找树，其插入，删除的核心都在2-nodes——\u0026gt;3-nodes-\u0026gt;4-nodes-\u0026gt;最后在分拆\n插入到2-nodes 当要要插入的节点是个2-nodes时，很简单，将其变为3-nodes即可 插入到3-nodes 当将一个值插入到3-nodes时，其变成4-nodes, 此时就要分拆。分拆后，树的高度+1 插入到父节点是2-nodes的3-nodes 由于插入到3-nodes会拆分，新生成的parent向上传递，而父节点是个2-nodes，因此父节点自然就组合成一个新的3-nodes 插入到父节点是3-nodes的3-nodes 同理，会向上两次传递。\n根节点的拆分 如果根节点本身已经是3-nodes, 在被向上拆分的子节点追加后会变成4-nodes, 此时根节点应该拆分，同时树的高度+1\n将临时的4-nodes拆分的过程是局部的，也就是说这种变换除了关联的节点及其链接，树的其他部分都不需要修改，因此每次变换我们只需要修改很小的部分，由此可说明当我们发现需要变换的节点时，这种变换时高效的。每次变换时，会将拆分出来的parent以及调整好结构的子树向外层传递。\n更重要的是，这种变换保持了树已经排好的顺序，并且保证树仍然是平衡的：也就是说从根到任何一个空节点的距离仍然是相等的。\n与BST比较，BST的增长是向下的，我们不断的添加子节点；平衡BST是从底部向上增长的，就是说我们在底部添加一个值，这个过程中如果有4-nodes形成就不断向上拆分，最终使root变成4-nodes,从而达到整个树高度+1\nProposition F: 查找和插入操作在N个元素构成的2-3查找树中需要访问最多lgN个节点。\n由此，分析2-3树与分析BST是不同的，因为2-3树的目的是为了确保最坏情况下的性能，而BST是分析其平均状况下的性能。在Symbol Table中由于我们没有办法去控制输入，所以分析最坏情况能够对性能做一个保证。\n红黑二叉树 为了实现2-3 tree，我们引入了红黑二叉树（red-black BST）,即用传统BST的Node结构加上一点额外信息来定义3-nodes：我们用red links 来将3-nodes中的两个元素链接起来，用black links来链接2-3tree中的每个节点。更具体的，我们将3-nodes用两个2-nodes用靠左的red link链接起来来表示。\n定义：红黑二叉树就是拥有Red links \u0026amp; Blank Links的BST并且满足如下限制：\nRed links靠左边 一个节点不能有两个Red links 整个树对于Black Links来说是完美平衡的：每个空链接到根都有相等的Blank Links数 颜色的表达 由于每个节点仅仅与其父节点相连接，我们在节点中增加一个是否是红色的属性，来表示其到其父节点的这个链接是否是Red links。\n旋转 在节点不断添加，合并，拆分的过程中，我们会碰到这些情况：Red Links并不靠左；一个节点有两个Red Links。 因此我们需要通过旋转来解决这些问题。旋转包括两种方式：左旋和右旋\n左旋： 当Red link靠右时，我们需要通过左旋将其转到左边。具体思路如下：当R靠右时，说明子节点是比父节点大的，所以旋转后应该将子节点放在上面，而原来的父节点作为左边的子节点，同时相应的其他调整 右旋： 右旋同理，调整前子节点比父节点小，因此要右旋的话，我们应该将子节点放在上面，原来的父节点放在右下 在旋转时，除了挂在的节点要调整外，还有一些信息要调整： 无论左旋还是右旋，都是将父，子节点进行颠倒，原先都是子节点中color=Red, 调整后，其变成了父节点，因此color=parent.color, 而原来父节点的color旋转后应该变为Red. (即旋转后，我们仍然保持red link 不变。 只是由于旋转后交换了父子节点，所以color=red的保存也跟着改变了) 旋转后，子节点放在了父节点的位置上，因此其n值就是原来父节点的n值，而原来父节点需要通过size(x) = size(left) + size(right) + 1重新计算一遍 插入 当我们在红黑树中插入一个新元素，我们一定是插入一个有Red link节点，然后在根据需要调整。 例如一个只有根的树，我们在其左边插入一个节点，由于R靠左，不需要调整；如果插入在其右边，此时我们就需要做一个左旋。 如果我们插入的是一个3-nodes, 那么就可能有三种情况\n新插入的值最大，那么就会形成一个中间元素有两个R子节点的情况——这种只需要要将其变成黑色即可 如果插入的值最小，就会形成中间元素的上下都是R的情况，此时我们要将其调整成正三角（上面的样子）——因此我们我们只要对上面的那根R做右旋，然后改变颜色即可 如果插入的值在中间，那就形成了中间的元素向上是R, 右下是R， 因此我们先最右下这个R进行左旋，变成2的样子，再继续操作即可 颜色的翻转 上面1的情况，我们需要做颜色的翻转，即将左右的两条红色变成黑色，Parent的设定成红色即可\n保持根的黑色 由于不断插入，最后颜色的翻转可能导致根的颜色是红色，所以我们需要插入元素后重新设定根是黑色的\n总结： 从上面1，2，3可以看出，操作是层层递进的。因此在实现中我们可以先检查右节点是否是R，进行左旋，然后检查左子节点及左子节点的子节点是否是红的，做右旋；最后做检查左右是否都是红的，做颜色翻转\n删除 对于删除，最重要的就是不能破坏其平衡性。如果以2-3树为思考，也就是说如果我们删除的地方是个2-nodes,删除后必然导致其下面的空链接长度改变，破坏了平衡。但是如果删除的位置是个3- nodes，那么即使我们删除掉这个元素，该位置仍然还有其他元素存在，我们只需要重新挂载一下子树就行，不会破坏平衡性。所以删除的核心就是如果能够确保我们在仍和情况下都是删除的3-nodes.\n红黑二叉树是通过插入时的拆分来保持平衡，那么在删除时我们就需要先做合并，将2-node合并成3-nodes或者4-node,删除完成后，再将多余的4-nodes重新拆分\n如何合并：\n如果是左右都是2-nodes, 那么不管parent是2-nodes还是3- nodes，我们都可以从parent找一个节点出来，和这两个子节点构成4- nodes，只要将左右link变红即可。 如果左右子节点一个是2- nodes， 一个是3- nodes，那么对于要扩展的那一变，我们只需要将parent拿下来和这个2-nodes拼成一个3- nodes，而从另一个3-nodes中取一个节点放到原来parent的位置上 删除最小值 有了上面合并的思路，我们要删除最小值，就意味着我们沿着最左边的一条路径不断做合并到底部，然后删除最小元素，进而原路返回，将路上碰到的4-nodes重新拆分掉\n如果当前节点的左孩子不是2- nodes，那么什么都不用做，继续向坐下走 如果左孩子是个2- nodes，并且其直接兄弟是个3- nodes，那么从3-nodes中移动一个元素，将2-node变成3-nodes 如果左孩子及其直接兄弟都是2- nodes，那么将这两个孩子以及parent中最小的元素一起构成一个4-nodes,此时parent将从4-nodes变成3-nodes或者从3-nodes变成2- nodes。 删除任意的值 有了上面删除最小值为基础，当我们删除任意值时，同样的道理，如果最终删除的位置在底部，那么我们直接删除就可以了，如果不是底部，此时我们将这个元素和比他大的最近的元素（即右子树的最小值）做交换。于是我们将问题转化为删除右子树的最小值。\n分析 Proposition G: 由N个元素组成的红黑二叉树其高度不高于2lgN\nProposition H：由N个元素组成的红黑二叉树从其根到节点的距离大约是～1.00lgN\n由此可见，红黑二叉树平均下来大约比BST快40%左右\n","date":"2022-01-27","img":"","permalink":"/zh-cn/guide/algo-search-balanced-bst/","series":[],"tags":["search","bst"],"title":"查找-平衡二叉树"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n对于查找，第一个直觉就是先排序，然后再从中找到需要的对象。那么为什么要将查找和排序分开来呢，一个重要的原因是对于查找我们需要一种结构能够保证我们既找的快，有能够不因为插入/删除时耗费大量的时间。\n对于符号表(symbol-table)来说，二叉树(Binary Search Trea \u0026ndash; BST)是一种理想的结构。\n定义 二叉查询树是一种二叉树，其每个节点包含有一个可比较的（Comparable）键值，并且规定任何一个节点的值都比其左子树的值大，比其右子树的值小。\n1private class Node { 2 private final Key key; 3 private Value value; 4 private Node left, right; 5 private int n; 6 7 public Node(Key key, Value value, int n) { 8 this.key = key; 9 this.value = value; 10 this.n = n; 11 } 12 } 要注意的一点： n 记录了当前节点下子节点的数量(包含自身), 因此对于节点x, 其size的公式为\n1size(x) = size(x.left) + size(x.right) + 1; n的用处在于我们可以通过x.left.n知道当前节点在整个序列中的位置，这对于实现rank等方法非常有用。\n搜索 二叉查询树的搜索是很直观的，通过递归的方式很好理解：将要查找的key和当前节点的key比较，如果小，则继续在左子树中查询，如果大，则在右子树中查询，如果相等则命中。如果左/右子树为空，则未命中。 如果在整个树中包含某个值，则称为命中（search hit）, 否则叫未命中(search miss)\n插入 插入同样很简单，从根开始不断向下走（小就走左边，大就走右边），知道走到底部，将节点插入到底部节点的下面就可以了。 这里要注意一下递归： 跟搜索一样，插入也同样使用了递归。我们不断使用递归走到目标所在之处，对于搜索来说，此时我们返回结果，每层递归将传递过来的结果返回出去。但是插入不一样，我们需要更新我们的子树，同时更新我们的节点数n。\n分析 二叉树的查找时间依赖于树的形状，也就是说依赖于输入的顺序。\n当二叉树是完全平衡的，那么其高度大约为～lgN, 在最坏的情况下，其高度为N, 通常情况下，树的形状还是比较接近平衡的。 在许多应用中，我们认为其输入都是随机的，那么分析BST就近似于我们分析QuickSort. 树的根就如同QS中的第一次分段，由此我们得出如下推论：\nProposition C: 对于随机的N个输入构造的BST, 查找命中平均大约需要～2lnN(也就是1.39lgN)次比较。 Proposition D: 插入或者查找未命中对于随机的N个输入构造的BST平均大约需要～2lnN(也就是1.39lgN)次比较\n跟顺序有关的其他方法以及删除 BST最重要的特征就是就是允许我们按键值的顺序保存，因此也就提供许多需要基于顺序的方法\n最大/最小值 Floor /Ceiling 对于Floor，我们递归的比较key与当前节点的键值，如果小于节点，那么在左子树中继续查找，如果相当，那就是这个节点，如果比节点大，那么我们就需要去右子树找，此时存在一种情况，如果右子树中找不到比key小的值，那这个节点就是返回值 Ceiling类似\nselect select 就是查找指定位置的节点。 此时我们就需要利用节点中n的信息。 由于一个节点所处的位置等于其左子树的n + 1, 所以我们将指定的值与左子树的n值比较，如果小，那么继续在左子树的查找，如果大，那么我们转而查找右子树，由于左边已经有n个元素了，所以其实我们在右子树中查找k -n - 1位置的元素。以此类推\nrank rank 查找元素的排名，仍然是递归的方式，如果key比当前节点小，那么在左子树去查找，如果相当，那么排名就等于小size(x.left), 如果大，那么其排名就是size(x.left) + 其在右子树的排名 + 1 注意一点，rank是从0开始的，而我们的节点的n默认是1（包含自己），所以一个节点的rank就是x.left\n删除最大/最小节点 删除最小节点是为删除任意节点做准备。 要删除最小节点思路很简单，不断在左子树中找到左子树为空的节点即可。删除时，用该节点的右子树来代替该节点。\n删除任意节点 删除节点仍然是通过递归先查找该节点。 删除时思路为用该节点右子树的最小节点来代替该节点，所以我们找到其右子树最小节点后问题转化为删除右子树的最小节点。\nProposition E: 对于BST, 在最坏的情况下，任何操作所需要的时间跟树的高度成正比。\n总结 BST实现比较容易，同时如果输入时比较随机的话能够提供很好的查询和插入效率。同时BST也提供了快速的rank, select, delete ,range query等方法，因此也受到欢迎。同时也要强调，其性能深受输入顺序的影响，在比较坏的情况下可能无法达成预期。 与快速排序比较，我们可以在快速排序前对数组先做一次随机操作，但是对于symbol table, 我们没办法这样做。\n因此我们需要引入平衡二叉树。 ","date":"2022-01-26","img":"","permalink":"/zh-cn/guide/algo-search-bst/","series":[],"tags":["search","bst"],"title":"查找-二叉树"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n快排由于实现起来简单，对各种类型的数据都有效，比大多数排序算法都效率高，因此可能是使用最广泛的排序算法。快排的两个显著特征：直接在数组上进行操作，不需要额外的内存；执行时间接近于NlgN, 同时结合这两点是其他算法（插入排序，选择排序，合并排序等）所不具备的。\n并且由于快排使用更短的内循环次数，因此在实际使用中效率可能更好。 快排的缺点在于在实现中有许多细节需要考虑到，否则可能大大的降低效率。\n实现 快排使用的是分治的思路，通过将数组分割成两个部分，然后独立的对两部分进行排序。\n","date":"2022-01-13","img":"","permalink":"/zh-cn/guide/algo-sort-quicksort/","series":[],"tags":["sort"],"title":"排序-快速排序"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n实现 这个算法的基本思想是合并：将两个排好序的数组合并成一个数组。 因此合并排序简单来说就是将一个数组分成两半，递归的将两半各自排好序，再将其合并成一个大的数组。 合并排序最重要的一个特点就是能保证执行时间是NlogN。缺点是需要跟N成比例的额外的空间。\n要实现合并排序，最直接的就是实现将两个数组合并成一个数组的方法：额外创建一个数组，从两个数组中不断取最小的对象放到这个存放数组中。 当要对一个大数组进行合并排序时，我们需要做大量的合并操作，因此每次创建新数组去保存结果也是一笔不小的开销。因此，如果能够直接在数组上操作，可以大大减小开销————想起来很简单，实现起来很复杂，特别是跟用额外数组做合并的方式比较起来的话。\n1public static void merge(Comparable[] a, int lo, int mid, int hi) { 2 // Merge a[lo..mid] with a[mid+1..hi]. 3 int i = lo, j = mid+1; 4 for (int k = lo; k \u0026lt;= hi; k++) // Copy a[lo..hi] to aux[lo..hi]. 5 aux[k] = a[k]; 6 7 for (int k = lo; k \u0026lt;= hi; k++) // Merge back to a[lo..hi]. 8 if (i \u0026gt; mid) a[k] = aux[j++]; 9 else if (j \u0026gt; hi ) a[k] = aux[i++]; 10 else if (less(aux[j], aux[i])) a[k] = aux[j++]; 11 else a[k] = aux[i++]; 12} 自顶而下的合并 上面的实现是著名的分治的例子：如果对两个子数组排序，那么将两个数组合并后就相当于对整个数组排序了。 要理解合并排序，需要理解调用的过程。假设排序a[0\u0026hellip;15], 需要调用a[0\u0026hellip;7] -\u0026gt;a[0\u0026hellip;3] -\u0026gt;a[0\u0026hellip;1] 从而调用merge(0, 1), 然后继续调用a[2\u0026hellip;3] 调用merge(2,3)如此反复。 我们可以看出，sort通过递归，提供了一组调用merge的顺序组，排序是通过不断的merge来实现的，而sort则指导什么时候对什么数组进行merge。\nProposition F: 对于长度为N的数组，自顶而下的合并排序使用1/2NlgN ～ NlgN之间次数的比较 证明：假设C(N)代表对长度为N的数组排序所需要的比较次数，我们有C(0)=C(1)=0。对于N\u0026gt;0,我们按照递归的思路，可以得到C(N)\u0026lt;= C(N/2) + C(N/2) + N, 其中第一个C(N/2)代表做半部排序的次数，第二个代表右半部的次数。 同时还有C(N) \u0026gt;= C(N/2) + C(N/2) + N/2, 这里N/2代表左半部（我们至少要比较一半） 如果N = 2n, 那么左半部和右半部都是2n-1。 由此我们推导出 C(2n) = 2C(2n-1) + 2n (后面略)\n换一种方式来理解Proposition F\n优化 对上面算法可以继续优化\n由于对小数组，插入排序是高效的，因此我们当递归到小数组（比如长度小于15）时，我们可以用插入排序来替代 在合并两个数组前先检查左半部的最后一个对象是否刚好小于等于右半部的第一个对象，如果是，我们就不用挨个比较了 消除额外的数组复制。 我们合并时，要不然先将对象复制到一个数组，然后合并回来，要不然将对象合并到一个数组，再复制回来。因此如果我们能够在一次合并时，将一个数组作为输入，一个作为输出，下次合并时将输出的数组作为输入，就能够减少一次复制 注意：以上的优化不是必须的，因为优化往往伴随着代码的复杂。解决一个问题时，我们应该首先选用最简单的办法，当遇到性能瓶颈时再考虑优化他。\n自下而上的合并 对于分治的思想，由于最终都是将大问题逐渐分解成最小的问题，所以我们可以自下而上，即现两个两个合并，然后四个四个合并，直到整个数组的合并。\n对于长度为2的幂的数组，自顶而下 和 自下而上效率时相同的，只是顺序相反， 但是其他长度则可能有效率上的差距\n排序算法的复杂度 学习合并算法的一个重要原因是因为通过他我们可以推导出排序算法的复杂度，即排序算法的极限。 研究算法的复杂度就是建立一个算法的计算模型。 对于以比较为基础的排序算法（插入，选择，合并排序都是此类， 还有不用通过比较来排序的算法），关键制约是：\n可以处理任意多的输入 我们除了在比较时能够知道两个对象的key之外得不到其他任何key的信息 Prop I. 没有以比较为基础的排序算法可以做到比较的次数少于lg(N!) ~ NlgN ","date":"2022-01-12","img":"","permalink":"/zh-cn/guide/algo-sort-mergesort/","series":[],"tags":["sort"],"title":"排序-合并排序"},{"categories":["算法"],"content":" 本文内容来自 Algorithhms 4th Edition\n前言 排序就是将对象按照一定的顺序排列。据统计，大约有30%的运算都是排序。 有三个理由让我们需要研究排序：\n研究排序算法是我们研究其他算法效率的基础 在研究过程中使用的技巧对于其他问题也是有效的 我们经常以排序作为解决其他问题的第一步 更重要的是排序算法本身是优雅，经典和高效的\n着手点 我们首先要定义什么是排序 验证。 我们需要验证我们算法的正确性。 即使我们已经测试过，我们也应该为每个算法提供一个检验函数isSorted() 执行时间。 我们首先由一些不同的排序算法得到每个算法的基本操作数量（例如比较，交换，对数组的读写等），然后由这些数据作为基础提出一个算法效率的假设，并且提供一个可以用来验证这个假设正确与否的工具， 内存使用。算法的内存使用情况和执行时间同样重要。排序算法通常可以分为两类：一种直接排序，基本不使用或者只用少量的栈； 另一种需要使用额外的内存来保存排好序的结果 数据类型：我们的排序代码对于任何实现了Comparable\u0026lt;T\u0026gt;的对象都是有效的 选择排序 这个排序很简单： 首先，找到最小的对象将其放在第一位，然后依次找到最小的放在第二，第三位，直到这个数组排好序。这个算法叫选择排序是因为它通过不断在剩余的对象中寻找最小的元素。\nProposition A: 算法使用~N2/2次比较和N次交换\n算法的特点：\n执行时间对于输入的数据不敏感。在一次循环中查找最小元素对于下次循环没有什么帮助，因此即使是一个排好序的数组仍要花费相同的时间进行排序。这是个极大的劣势 数据的移动很少，最多交换N次，相对于数组长度是线性的。其他算法都大大多于这个数量 插入排序 插入排序类似于我们对扑克牌的排序，每次将一张牌插入已经排好序的牌的对应的位置之中。具体实现中，我们需要通过将所有大的元素向右移动一个位置来为当前的元素空出位置。 在插入排序里，当前位置左边的元素都是已经排好序的，但是他们的位置不一定是最终的位置，因为继续排序的过程中可能会为更小的元素腾位置。但是当到达最后的位置时，整个数组是排好序的。 与选择排序不同，插入排序的执行时间跟数组最初的顺序有关。如果数组本身已经排好序，那么其执行会很快\nProposition B: 执行时间： ～N2/4次比较和～N2/4次交换，最坏的情况下～N2/2次比较和～N2/2次交换\n想象一下，最坏的情况，就是一个选择排序，因为每次都要将该元素放到最左边， 平均下来，如果是比较一半，那么就是～N2/4\n对于已经排好序的数组，插入排序很适合 更普遍的，我们考虑部分排序的数组：假设我们定义一个颠倒是指数组中一对元素顺序错误，例如E X A M P L E有11对颠倒：E-A, X-A, X-M, X-P, X-L, X-E, M-L, M-E, P-L, P-E, and L-E。如果数组中这样的颠倒的个数少于数组长度的常数倍，我们就说这个数组是部分排序的。 常见的部分排序的数组的例子：\n一个数组中，每个元素离正确的位置都不太远 一个小的数组在一个大的已经排好序的数组之后 一个数组中只有几个元素位置不正确 插入排序对于上述部分排序的数组是高效的，而选择排序则不。事实上，对于已经部分排序的数组，插入排序比其他排序算法更高效。\nProposition C: 插入排序中交换的次数等于颠倒的数量，比较的次数最少是颠倒的数量，最多是颠倒的数量加上数组的长度减去1.\n总结：插入排序对于部分排序数组和小数组是极好的算法，不仅仅是因为这两类数组经常碰到，还因为对于许多高级算法，这两类数组通常会作为中间结果出现。\n比较两种排序算法 要比较算法，我们需要遵从下面的步骤\n实现算法 分析每个算法的基本属性（Proposition A, B, C） 形成算法性能比较的推论 (Proposition D) 执行测试数据来验证这个推论 (SortCompare) 具体来说：\n实现算法： 略\n分析 为输入对象建立合适的model. 对于排序，我们假设数组是随机的，并且key是不重复的。 因此对于key会重复的应用需要慎重对待\n推论 Property D: 选择排序和插入排序对于随机的，key不同的数组的执行时间是数组长度的平方和一个小的，常数因子的积\n验证： 我们实现一个SortCompare类，这个类很简单，就是输入生成随机数组的长度(N)和重复次数(T)，计算不同算法的话费时间总和进而比较\n1public class SortCompare { 2 3 public static double time(String alg, Double[] a) { /* See text. */ } 4 5 public static double timeRandomInput(String alg, int N, int T) { // Use alg to sort T random arrays of length N. 6 double total = 0.0; 7 Double[] a = new Double[N]; 8 for (int t = 0; t \u0026lt; T; t++) { 9 // Perform one experiment (generate and sort an array). 10 for (int i = 0; i \u0026lt; N; i++) 11 a[i] = StdRandom.uniform(); 12 total += time(alg, a); 13 14 } 15 return total; 16 } 17 18 public static void main(String[] args) { 19 String alg1 = args[0]; 20 String alg2 = args[1]; 21 int N = Integer.parseInt(args[2]); 22 int T = Integer.parseInt(args[3]); 23 double t1 = timeRandomInput(alg1, N, T); // total for alg1 24 double t2 = timeRandomInput(alg2, N, T); // total for alg2 25 StdOut.printf(\u0026#34;For %d random Doubles\\n %s is\u0026#34;, N, alg1); 26 StdOut.printf(\u0026#34; %.1f times faster than %s\\n\u0026#34;, t2/t1, alg2); 27 } 28} 贝壳排序（ShellSort） 插入排序对于大的，无序的数组排序之所以慢是因为每次只能对相邻的元素进行交换，一次只能移动一位。例如如果最小的元素排在最后，则需要移动n-1次。贝壳排序是对插入排序的扩展。通过允许对间隔较远的元素进行交换，形成多个部分排序的数组，最后用插入排序进行有效的处理。\n具体思路：将数组以间隔h长度分成多个子数组，对每个子数组进行排序。例如以间隔4为单位，那么0，4，8。。。为一组，1，5，9\u0026hellip;.为另一组。这个方式的优点在于我们可以将交换的间隔从插入排序的1位变成了h位。同时每个子数组的长度变小了，当h足够大时，就变成了对小数组的排序\u0026ndash;插入排序对于小数组是有效率的。 然后我们逐渐减小h的值，在这个过程中我们是不断的对部分排序的数组进行排序，因此也能保证效率。当h=1时即完成了排序。\n1public void sort(T[] a) { 2 int n = a.length; 3 int h = 1; 4 while (h \u0026lt; n / 3) h = 3 * h + 1; 5 while (h \u0026gt;= 1) { 6 for (int i = h; i \u0026lt; n; i++) { 7 for (int j = i; j \u0026gt;= h \u0026amp;\u0026amp; less(a[j], a[j-h]); j -= h) { 8 exch(a, j, j-h); 9 } 10 } 11 h = h / 3; 12 } 13 } 这里有几个值得注意的点：\n代码中按照3倍进行分割，也就是说1， 4， 13， 40\u0026hellip; 假设长度为16的数组，第一次h=13, 实际上就是对数组的首尾进行了排序。13\u0026lt;-\u0026gt;0, 14\u0026lt;-\u0026gt;1,15\u0026lt;-\u0026gt;2。 第二次循环，h=4,于是从index=4开始到末尾，间隔4个排序。依次类推 假设按照4倍分割，则为1，5，21，。。。，对于大数组，比如10000个，跟3区别不大，甚至按照5倍分割区别不大，（执行时间接近）,以下是按照不同倍数分割的执行时间 1shell----split = 2, to 1000000 random array, spent 816 millis 2shell----split = 3, to 1000000 random array, spent 712 millis 3shell----split = 4, to 1000000 random array, spent 738 millis 4shell----split = 5, to 1000000 random array, spent 704 millis 5shell----split = 6, to 1000000 random array, spent 718 millis 6shell----split = 7, to 1000000 random array, spent 768 millis 7shell----split = 8, to 1000000 random array, spent 784 millis 8shell----split = 9, to 1000000 random array, spent 841 millis 9shell----split = 10, to 1000000 random array, spent 827 millis 10shell----split = 11, to 1000000 random array, spent 825 millis 11shell----split = 12, to 1000000 random array, spent 884 millis 12shell----split = 13, to 1000000 random array, spent 902 millis 13shell----split = 14, to 1000000 random array, spent 922 millis 14shell----split = 15, to 1000000 random array, spent 951 millis 15shell----split = 16, to 1000000 random array, spent 1021 millis 16shell----split = 17, to 1000000 random array, spent 1022 millis 17shell----split = 18, to 1000000 random array, spent 1116 millis 18shell----split = 19, to 1000000 random array, spent 1145 millis 从上面可以看出，当split比较小时，我们最开始循环的子数组数量比较少，速度比较快。 想想对于16长度，3倍分割，第一次从13开始，子数组长度是2， 如果是4倍分割，则从5开始，那么排在10以后的需要排3个， 由此可见， 小数组和部分排序的数组是影响插入排序效率的关键 3. 贝壳排序的核心就是对于h,用插入排序来独立的处理每个h 序列。注意每次h是开头，所以要求j\u0026gt; h\n如何决定分割量的大小？不同的值倍研究过，但还没有找到最佳的值，因此按例子中3倍来是可行的\n执行时间 贝壳排序的实行时间不在是N2的倍数。 例如上面算法最坏情况下是N3/2\n简单的修改算法就可以打破N2的限制，这就是算法研究的有趣之处\nProperty E : 如果以1，4，13，40，121，364。。。为增量，贝壳算法的比较次数不超过增量的N倍的一小部分\n总结 贝壳算法代码简单，代码量少。对于中大型数组效率比较高，同时不额外使用内存\n","date":"2022-01-10","img":"","permalink":"/zh-cn/guide/algo-sort-elementory/","series":[],"tags":["sort"],"title":"排序-基本排序"},{"categories":["工具"],"content":"一直觉得自己总该写些什么，特别是在网上浏览别人的文档时，总会不自觉的被各种分享所吸引。 模糊中记得一句话，作为一个IT从业人员，重要的一点就是在网上留下自己的印记。好吧， 是否留下印记无所谓， 不过想想将来有一天翻看这些文档，犹如自己翻看曾经的日记，感觉也是挺美妙的。\n大概两年前就打算这样弄过， 但是懒总是阻碍人类前进的一大阻力。 在github上开通了git page, 写下自己的大名之后，时间就静止了，一晃两年过去。\n继续两年未竟之事业 —— 大约等于从头开始。\n选择Host 当然是选择免费的，github提供了page。 如果是用码云的，似乎也有相应的page,道理类似。\n所谓git page, 是github为每个人保留了一个特殊的repo, 就是你的用户名+ github.io. 创建好这个repo, 例如我的 yaoyuqi.github.io. 在里面更新静态html, 就可以访问：如http://yaoyuqi.github.io\n建站方式 由于是静态页面，所以我们需要一个方案来将我们的文档有组织有结构的生成html。\n手写是不可能的，这辈子都不可能的。\n翻了翻，大约流行的是这三个jekyll, hexo, hugo. 这三个都是比较完整的框架，内置server, 也就是说可以运行后在本地通过浏览器访问本地的网页，同样也支持各种插件安装。最后生成需要上传的页面\n简单比较 方案 语言 特点 jekyll Ruby 应该很老牌，github默认建git page时就推荐用这个。 应该各种插件之类很全。 编译速度慢一点 hexo Node 用node搞的，好像是台湾的达人弄的，所以中文文档丰富 hugo go 编译速度快。 看了看文档页，目测文档数量是hexo的两倍 —— 复杂 我本打算选用jekyll, 可以按照官方文档安装后运行报错， 搜了一下，应该是mac的m1芯片在调用sass的一个解析器时出错。对于这种错误，如果没有很快速的搜到解决方案，那就放弃，折腾起来太费时间\n——所以Jekylly被我放弃了。\n于是我拥抱了hugo\nhugo构建 很简单\n将repo clone下来 按照hugo文档安装。 创建一个页面，运行hugo server -D ，即可在本地查看预览 推送上去就能访问了 突然发现我的这个guide很水。。。 😄😄😄😄\n其实最后一点不准确，推送上去是看不到刚才创建的文章的，甚至连网页都打不开，因为整个repo都变成了包含hugo的一整套库，git page是不认识的，记得吗，人家只认静态页面。\n我们推送的应该是hugo编译好的结果的文件。例如xxx.html/ xxx.js/ xxxx.cs\n这些文件在哪里呢？ 其实在public目录下，运行hugo 就会生成，因此我们要上传的其实是这个public文件夹。\n更进一步 参考hugo的部署文档，发现里面指导使用github的actions来发布。 关于github的actions, 可以参考这篇文章 简单的说，这个actions就类似jenkins, github提供了一些虚拟机环境，你可以通过配置脚本在执行一些操作。\n以我们发布文章来说，我们需要当提交整个repo后，github能够帮我们编译生成静态页面，然后将其放到我们git page的页面里。 这里再说一下，github为这类提供了一个专用分支，gh-page, 也就是说按照部署文档来最后我们可以看到repo多了一个gh-page分支，里面就是public的内容。\n使用github的actions很单间，就是在repo下创建一个.workflows目录，再放一个xx.yml，这个xx.yml就是配置文件。每次提交，github就会触发这个action\n万事具备，于是这篇文章就诞生了。\n","date":"2021-12-11","img":"","permalink":"/zh-cn/guide/use-hugo-with-github-page/","series":[],"tags":[""],"title":"Use Hugo With Github Page"},{"categories":null,"content":"你好，世界。\n这是一个新的开始，我相信明天总是很美好的。\n","date":"2021-12-09","img":"","permalink":"/zh-cn/posts/my-first-post/","series":null,"tags":null,"title":"My First Post"},{"categories":null,"content":" Tian Yuan\u0026rsquo;s husband and Pingping \u0026amp; Anan\u0026rsquo;s father. More than 10 years in software developing. Mainly in Java, Php, Js Plantyful experience in Lavarel, Spring, React. Others including Javafx, Android ","date":"2021-12-09","img":"","permalink":"/zh-cn/about/","series":null,"tags":null,"title":"About"},{"categories":null,"content":"","date":"0001-01-01","img":"","permalink":"/zh-cn/offline/","series":null,"tags":null,"title":"Offline"},{"categories":null,"content":"","date":"0001-01-01","img":"","permalink":"/zh-cn/contact/","series":null,"tags":null,"title":"联系我们"}]